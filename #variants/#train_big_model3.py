# import os
# from pathlib import Path
import random
import torch
import multiprocessing as mp
import pandas as pd
import albumentations as A
from PIL import Image
from pathlib import Path
from datetime import datetime
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset

from transformers import (
    TrOCRProcessor,
    VisionEncoderDecoderModel,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    default_data_collator,
    TrainerCallback,
)
from transformers.trainer_utils import get_last_checkpoint
import evaluate
import numpy as np
import logging
import zipfile
import io
from collections import OrderedDict
import os

# from src.utils.start_tensorboard import start_tensorboard


logging.basicConfig(level=logging.INFO)

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M')
OUTPUT_DIR = Path(rf"D:\DOC\2025-11-trocr_train\output\{TIMESTAMP}")
MODEL_CHECKPOINT = "microsoft/trocr-small-handwritten"
CUSTOM_LOADER_DATASET = "ImageNet"
VALIDATION_SPLIT_SIZE = 0.05
RANDOM_SEED = 42
final_csv_path = Path(rf"d:\datasets\rus\dataset_full_index.csv")  # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–∞—Ç–∞—Å–µ—Ç–∞

LOG_DIR = Path(rf"{OUTPUT_DIR}\logs")
MAX_CACHE_ZIP_FILES = 50

# --- –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞–π–ø–ª–∞–π–Ω –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π ---
train_transforms = A.Compose([
    A.Rotate(limit=5, p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),
    A.Affine(translate_percent=0.0625, scale=0.1, rotate=5, p=0.5),
    A.Blur(blur_limit=3, p=0.2),
])


# --- –ö–ª–∞—Å—Å –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö ---
class BigCyrillicHandwrittenDataset(Dataset):
    def __init__(self, df, processor, transforms=None, max_target_length=128, max_cache_size=8):
        """
        :param df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ 'zip_path' –∏ 'image_path' (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π)
        :param processor: TrOCRProcessor
        :param max_cache_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ ZIP-–∞—Ä—Ö–∏–≤–æ–≤ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –≤ RAM
        """
        self.df = df
        self.processor = processor
        self.transforms = transforms
        self.max_target_length = max_target_length
        self.max_cache_size = max_cache_size
        self.cache = OrderedDict()  # –ö—ç—à –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ ZipFile –∏–ª–∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (–±–∞–π—Ç–æ–≤)
        logging.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω BigDataset —Å –∫—ç—à–µ–º –Ω–∞ {max_cache_size} –∞—Ä—Ö–∏–≤–æ–≤.")

    def __len__(self):
        return len(self.df)

    def _get_archive_data(self, zip_path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–ª–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∞—Ä—Ö–∏–≤–∞ –∏–∑ –∫—ç—à–∞ (LRU-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å: {–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ_–∏–º—è_—Ñ–∞–π–ª–∞: –±–∞–π—Ç—ã_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è}
        """
        zip_path_str = str(zip_path)

        if zip_path_str in self.cache:
            # 1. –ö—ç—à-–•–∏—Ç: –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ –∫–æ–Ω–µ—Ü (—Å–∞–º—ã–π —Å–≤–µ–∂–∏–π)
            self.cache.move_to_end(zip_path_str)
            return self.cache[zip_path_str]

        # 2. –ö—ç—à-–ú–∏—Å: –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–π –∞—Ä—Ö–∏–≤
        logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ ZIP-–∞—Ä—Ö–∏–≤–∞ –≤ RAM: {zip_path_str}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –∫—ç—à–∞
        if len(self.cache) >= self.max_cache_size:
            # LRU: –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π —ç–ª–µ–º–µ–Ω—Ç (–ø–µ—Ä–≤—ã–π)
            lru_key, _ = self.cache.popitem(last=False)
            logging.warning(f"–ö—ç—à –∑–∞–ø–æ–ª–Ω–µ–Ω ({self.max_cache_size}). –£–¥–∞–ª–µ–Ω —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π –∞—Ä—Ö–∏–≤: {lru_key}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ ZIP –≤ —Å–ª–æ–≤–∞—Ä—å –±–∞–π—Ç–æ–≤
        new_cache_entry = {}
        try:
            with zipfile.ZipFile(zip_path, 'r') as zf:
                for name in zf.namelist():
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–∞–ø–∫–∏ –∏ –Ω–µ-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
                    if name.lower().endswith(('.jpg', '.jpeg', '.png')):
                        new_cache_entry[name] = zf.read(name)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –∏–ª–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–∏ ZIP {zip_path}: {e}")
            raise

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π –∞—Ä—Ö–∏–≤ –≤ –∫—ç—à
        self.cache[zip_path_str] = new_cache_entry
        return new_cache_entry

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        zip_path = row['zip_path']
        internal_file_name = row['image_path']
        text = row['text']

        # 1. –ü–æ–ª—É—á–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∞–π—Ç—ã –∞—Ä—Ö–∏–≤–∞
        archive_data = self._get_archive_data(zip_path)

        # 2. –ü–æ–ª—É—á–∞–µ–º –±–∞–π—Ç—ã –Ω—É–∂–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if internal_file_name not in archive_data:
            logging.error(f"–§–∞–π–ª {internal_file_name} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∞—Ä—Ö–∏–≤–µ {zip_path}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º None –∏–ª–∏ –ø–æ–¥–Ω–∏–º–∞–µ–º –æ—à–∏–±–∫—É, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∂–µ–ª–∞–µ–º–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è
            return self.__getitem__(
                random.randint(0, len(self.df) - 1))  # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: –≤–∑—è—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π –¥—Ä—É–≥–æ–π

        image_bytes = archive_data[internal_file_name]

        # 3. –î–µ–∫–æ–¥–∏—Ä—É–µ–º –±–∞–π—Ç—ã –≤ –æ–±—ä–µ–∫—Ç PIL.Image
        image = Image.open(io.BytesIO(image_bytes)).convert("RGB")

        # 4. –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (–∫–∞–∫ –≤ –≤–∞—à–µ–º –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ)
        if self.transforms:
            image_np = self.transforms(image=np.array(image))['image']
            image = Image.fromarray(image_np)

        # 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º (–∫–∞–∫ –≤ –≤–∞—à–µ–º –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ)
        pixel_values = self.processor(images=image, return_tensors="pt").pixel_values
        labels = self.processor.tokenizer(
            text,
            padding="max_length",
            max_length=self.max_target_length,
            truncation=True,
        ).input_ids

        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]

        return {"pixel_values": pixel_values.squeeze(), "labels": torch.tensor(labels)}


def compute_metrics(pred, processor):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ CER"""
    labels_ids = pred.label_ids
    pred_ids = pred.predictions
    pred_ids[pred_ids == -100] = processor.tokenizer.pad_token_id
    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)

    cer_metric = evaluate.load("cer")
    cer = cer_metric.compute(predictions=pred_str, references=label_str)
    return {"cer": cer}


class EnhancedValidationCallback(TrainerCallback):
    def __init__(self, checkpoint_dir, processor, log_every=100, num_samples=5, early_stopping_patience=3):
        """
        callback –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.processor = processor
        self.log_every = log_every
        self.num_samples = num_samples
        self.early_stopping_patience = early_stopping_patience
        self.best_cer = float('inf')
        self.epochs_no_improve = 0
        self.writer = None

    def init_writer(self, logs):
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SummaryWriter"""
        if self.writer is None and 'tensorboard' in logs:
            self.writer = logs['tensorboard']
            print(f"üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ª–æ–≥–≥–µ—Ä TensorBoard")

    def on_evaluate(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        metrics = kwargs.get('metrics', {})
        global_step = state.global_step

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ –Ω–∞—à —à–∞–≥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        if global_step % self.log_every != 0:
            return

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
        self.init_writer(kwargs.get('logs', {}))

        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        cer = metrics.get('eval_cer', float('inf'))

        # –õ–æ–≥–∏—Ä—É–µ–º CER
        if self.writer:
            self.writer.add_scalar("Val/cer", cer, global_step)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if cer < self.best_cer:
            self.best_cer = cer
            self.epochs_no_improve = 0

            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            best_model_dir = self.checkpoint_dir / "best_cer_model"
            best_model_dir.mkdir(parents=True, exist_ok=True)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            kwargs['model'].save_pretrained(best_model_dir)
            self.processor.save_pretrained(best_model_dir)

            if args.local_rank in [-1, 0]:  # –¢–æ–ª—å–∫–æ –¥–ª—è –≥–ª–∞–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
                print(f"üéØ –ù–æ–≤—ã–π –ª—É—á—à–∏–π CER: {cer:.4f}. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {best_model_dir}")
        else:
            self.epochs_no_improve += 1
            if self.epochs_no_improve >= self.early_stopping_patience:
                print(f"‚ö†Ô∏è CER –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è {self.epochs_no_improve} —ç–ø–æ—Ö –ø–æ–¥—Ä—è–¥")

        # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
        if args.local_rank in [-1, 0]:
            print(
                f"Validation {datetime.now().strftime('%Y-%m-%d_%H-%M')} @ step {global_step} - CER: {cer:.4f} | Best CER: {self.best_cer:.4f}")

    def on_train_end(self, args, state, control, **kwargs):
        """–ó–∞–∫—Ä—ã–≤–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
        if self.writer:
            self.writer.close()
            print("‚úÖ –õ–æ–≥–≥–µ—Ä TensorBoard –∑–∞–∫—Ä—ã—Ç")


def load_model_and_processor():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ '{MODEL_CHECKPOINT}'...")

    processor = TrOCRProcessor.from_pretrained(MODEL_CHECKPOINT)
    model = VisionEncoderDecoderModel.from_pretrained(MODEL_CHECKPOINT)

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
    model.config.pad_token_id = processor.tokenizer.pad_token_id
    model.config.vocab_size = model.config.decoder.vocab_size
    model.config.decoder.dropout = 0.3
    model.config.decoder.attention_dropout = 0.3

    return model, processor


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ CUDA
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"

    # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ CUDA
    torch.cuda.empty_cache()

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    model, processor = load_model_and_processor()
    model = model.to(device)
    print("\n‚úÖ –ú–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω—ã.")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv(final_csv_path)
    train_df, eval_df = train_test_split(df, test_size=VALIDATION_SPLIT_SIZE, random_state=RANDOM_SEED)
    train_df = train_df.sample(10000, random_state=RANDOM_SEED)  # 10–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
    eval_df = eval_df.sample(2000, random_state=RANDOM_SEED)  # 2–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏

    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = BigCyrillicHandwrittenDataset(
        df=train_df,
        processor=processor,
        transforms=train_transforms,
        max_cache_size=50
    )

    eval_dataset = BigCyrillicHandwrittenDataset(
        df=eval_df,
        processor=processor,
        max_cache_size=50
    )

    print(
        f"\n–î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º BigCyrillicHandwrittenDataset. –û–±—É—á–µ–Ω–∏–µ: {len(train_dataset)}, –í–∞–ª–∏–¥–∞—Ü–∏—è: {len(eval_dataset)}")
    print("‚úÖ Dataset –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã.")

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    training_args = Seq2SeqTrainingArguments(
        output_dir=str(OUTPUT_DIR),
        predict_with_generate=True,
        per_device_train_batch_size=8,  # –£–≤–µ–ª–∏—á—å—Ç–µ –µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏
        per_device_eval_batch_size=8,
        # fp16=True,  # –í–∫–ª—é—á–∏—Ç–µ –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        gradient_accumulation_steps=4,  # –î–ª—è —ç–º—É–ª—è—Ü–∏–∏ –±–æ–ª—å—à–µ–≥–æ batch_size

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging_dir=str(LOG_DIR),
        logging_strategy="steps",
        logging_steps=100,
        eval_strategy="steps",
        eval_steps=500,
        save_strategy="steps",
        save_steps=500,
        save_total_limit=3,
        report_to=["tensorboard"],

        # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        num_train_epochs=10,
        learning_rate=1e-6,
        weight_decay=0.01,
        warmup_ratio=0.1,
        lr_scheduler_type="linear",

        # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é
        load_best_model_at_end=True,
        metric_for_best_model="cer",
        greater_is_better=False,

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging_first_step=True,
        logging_nan_inf_filter=False,
        eval_accumulation_steps=5,
        dataloader_num_workers=0,  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ 0 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å multiprocessing
        remove_unused_columns=False,
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ callback
    callback = EnhancedValidationCallback(
        checkpoint_dir=OUTPUT_DIR,
        processor=processor,
        log_every=200,
        num_samples=5,
        early_stopping_patience=5
    )

    # –§—É–Ω–∫—Ü–∏—è compute_metrics —Å –∑–∞–º—ã–∫–∞–Ω–∏–µ–º
    def compute_metrics_wrapper(pred):
        return compute_metrics(pred, processor)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics_wrapper,
        tokenizer=processor.feature_extractor,  # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ feature_extractor –∫–∞–∫ tokenizer
        callbacks=[callback],
        data_collator=default_data_collator,
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    resume_training = bool(get_last_checkpoint(OUTPUT_DIR))

    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    print(f"\nüöÄ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï! {datetime.now().strftime('%Y-%m-%d_%H-%M')}")
    print(f"–õ–æ–≥–∏ TensorBoard –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –≤: {LOG_DIR}")

    try:
        trainer.train(resume_from_checkpoint=resume_training)

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! {datetime.now().strftime('%Y-%m-%d_%H-%M')} –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å...")
        trainer.save_model(str(OUTPUT_DIR / "best_model"))
        processor.save_pretrained(str(OUTPUT_DIR / "best_model"))
        print(f"üéâ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {OUTPUT_DIR / 'best_model'}")

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ...")
        trainer.save_model(str(OUTPUT_DIR / "interrupted_model"))
        processor.save_pretrained(str(OUTPUT_DIR / "interrupted_model"))
        print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {OUTPUT_DIR / 'interrupted_model'}")

    except torch.cuda.OutOfMemoryError:
        print("\n‚ùå –û—à–∏–±–∫–∞: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ CUDA. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
        print("1. –£–º–µ–Ω—å—à–∏—Ç—å batch_size")
        print("2. –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        print("3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gradient_checkpointing")
        print("4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fp16/mixed precision")

    except Exception as e:
        print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        raise


if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–µ—Ç–æ–¥–∞ multiprocessing
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass  # –ú–µ—Ç–æ–¥ —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω

    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
    main()
