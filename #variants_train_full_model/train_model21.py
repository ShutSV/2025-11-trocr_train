"""
—Å–∫—Ä–∏–ø—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ microsoft-small-handwritten –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
"""

from pathlib import Path
from datetime import datetime
import random
import time
import torch
from torch.utils.tensorboard import SummaryWriter
from transformers import (
    VisionEncoderDecoderModel,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    default_data_collator,
    TrainerCallback,
)
from transformers.trainer_utils import get_last_checkpoint
import evaluate

from dataset_rus21 import MODEL_NAME, processor, train_dataset, val_dataset


TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M')
OUTPUT_DIR = Path(rf"D:\DOC\2025-11-trocr_train\output\{TIMESTAMP}")
LOG_DIR = Path(rf"{OUTPUT_DIR}\logs")

model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)
model.decoder.resize_token_embeddings(len(processor.tokenizer))

model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–≤–∞–ª–∏–¥–∞—Ü–∏–∏)
model.config.max_length = 64
model.config.early_stopping = True
model.config.no_repeat_ngram_size = 3
model.config.length_penalty = 2.0
model.config.num_beams = 4

cer_metric = evaluate.load("cer")

def main():

    def compute_metrics(pred):
        labels_ids = pred.label_ids
        pred_ids = pred.predictions
        pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)  # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
        labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id
        label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)
        cer = cer_metric.compute(predictions=pred_str, references=label_str)
        # –ü—Ä–æ—Å—Ç–æ –ø–µ—á–∞—Ç–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å—Ä–∞–∑—É –∑–¥–µ—Å—å
        print(f"\n–ü—Ä–∏–º–µ—Ä—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (CER: {cer:.4f}):")
        for i in range(min(3, len(pred_str))):
            print(f"  True: '{label_str[i]}'")
            print(f"  Pred: '{pred_str[i]}'")
            print()
        return {"cer": float(cer)}

    # ==============================
    # TrainingArguments (i9 185H RTX4000ada)
    # ==============================

    training_args = Seq2SeqTrainingArguments(
        output_dir=str(OUTPUT_DIR),
        predict_with_generate=True,
        per_device_train_batch_size=64,  # 64 –¥–ª—è RTX4000ada, 48 –¥–ª—è T4 –∏ L4, 96 –¥–ª—è –ê100 (VRAM 26 –∏–∑ 40)
        per_device_eval_batch_size=64,  # 96 –¥–ª—è RTX4000ada
        fp16=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è

        # --- –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
        logging_dir=str(LOG_DIR),
        logging_strategy="steps",
        logging_steps=50,  # –ß–∞—Å—Ç–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–ª–∞–≤–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤
        eval_strategy="steps",
        eval_steps=500,    # –ß–∞—Å—Ç–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        save_strategy="steps",
        save_steps=500,
        save_total_limit=3,
        report_to=["tensorboard"],

        # --- –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã ---
        # num_train_epochs=10,  # –¢–∞–∫ –∫–∞–∫ –¥–∞—Ç–∞—Å–µ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π/–ø–æ—Ç–æ–∫–æ–≤—ã–π, –ª—É—á—à–µ –∑–∞–¥–∞–≤–∞—Ç—å —à–∞–≥–∏, –∞ –Ω–µ —ç–ø–æ—Ö–∏
        max_steps=300_000,
        learning_rate=1e-5,
        weight_decay=0.01,
        warmup_steps=200,
        lr_scheduler_type="linear",
        # label_smoothing_factor=0.1,  # –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –±–æ—Ä–µ—Ç—Å—è —Å —Å–∞–º–æ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –≤ –ø—Ä–æ–±–µ–ª–∞—Ö

        # --- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é ---
        load_best_model_at_end=True,
        metric_for_best_model="cer",
        greater_is_better=False,

        # --- –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
        logging_first_step=True,  # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–π —à–∞–≥
        logging_nan_inf_filter=False,  # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è
        eval_accumulation_steps=3,  # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∫–∏
        dataloader_pin_memory=torch.cuda.is_available(),  # –£—Å–∫–æ—Ä–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        dataloader_num_workers=4,    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ - –û–¢–ö–õ–Æ–ß–ò–¢–¨ –¥–ª—è [i9 185H]
        dataloader_prefetch_factor=64,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π, –∑–∞–≥—Ä—É–∂–∞–µ–º—ã—Ö –∫–∞–∂–¥—ã–º worker'–æ–º –∑–∞—Ä–∞–Ω–µ–µ - –û–¢–ö–õ–Æ–ß–ò–¢–¨ –¥–ª—è [i9 185H]
        remove_unused_columns=False,  # –î–ª—è IterableDataset –Ω—É–∂–Ω–æ —è–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å, —á—Ç–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª–∏–Ω–∞

        # --- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        eval_delay=0,  # - –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å—Ä–∞–∑—É
        dataloader_drop_last=False,  # –Ω–µ –æ—Ç–±—Ä–∞—Å—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á
        disable_tqdm=False,  # –í–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä—ã
    )


    class FreezeEncoderCallback(TrainerCallback):
        def __init__(self, unfreeze_step=180):
            self.unfreeze_step = unfreeze_step
            self.is_unfrozen = False

        def on_step_begin(self, args, state, control, **kwargs):
            model = kwargs['model']

            # –ù–∞ —Å–∞–º–æ–º –ø–µ—Ä–≤–æ–º —à–∞–≥–µ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —ç–Ω–∫–æ–¥–µ—Ä
            if state.global_step == 0:
                for param in model.encoder.parameters():
                    param.requires_grad = False
                print(f"‚ùÑÔ∏è {datetime.now().strftime('%Y-%m-%d_%H-%M')} –≠–Ω–∫–æ–¥–µ—Ä –∑–∞–º–æ—Ä–æ–∂–µ–Ω –Ω–∞ –ø–µ—Ä–≤—ã–µ {self.unfreeze_step} —à–∞–≥–æ–≤.")

            # –ü—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –Ω—É–∂–Ω–æ–≥–æ —à–∞–≥–∞ ‚Äî —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º
            if state.global_step >= self.unfreeze_step and not self.is_unfrozen:
                for param in model.encoder.parameters():
                    param.requires_grad = True
                self.is_unfrozen = True
                print(f"üî• {datetime.now().strftime('%Y-%m-%d_%H-%M')} –®–∞–≥ {state.global_step}: –≠–Ω–∫–æ–¥–µ—Ä —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω. –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ.")


    class MemoryOptimizationCallback(TrainerCallback):
        def __init__(self):
            self.last_log_time = time.time()

        def on_step_end(self, args, state, control, **kwargs):
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
            current_time = time.time()
            if current_time - self.last_log_time > 1200:  # –ö–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
                torch.cuda.empty_cache()
                self.last_log_time = current_time
                print(f"üßπ {datetime.now().strftime('%Y-%m-%d_%H-%M')} –û—á–∏—â–µ–Ω–∞ –ø–∞–º—è—Ç—å GPU –ø–æ—Å–ª–µ 20 –º–∏–Ω—É—Ç")

        def on_evaluate_end(self, args, state, control, **kwargs):
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            print(f"üßπ {datetime.now().strftime('%Y-%m-%d_%H-%M')} –û—á–∏—â–µ–Ω–∞ –ø–∞–º—è—Ç—å GPU –ø–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")

    class EnhancedValidationCallback(TrainerCallback):
        def __init__(self, checkpoint_dir, processor, log_every=100, num_samples=5, early_stopping_patience=3):
            """
            callback –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

            :param checkpoint_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π
            :param processor: –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
            :param log_every: –ß–∞—Å—Ç–æ—Ç–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–≤ —à–∞–≥–∞—Ö)
            :param num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            :param early_stopping_patience: –¢–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            """
            self.checkpoint_dir = Path(checkpoint_dir)
            self.processor = processor
            self.log_every = log_every
            self.num_samples = num_samples
            self.early_stopping_patience = early_stopping_patience
            self.best_cer = float('inf')
            self.epochs_no_improve = 0
            # self.writer = None  # –ë—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
            self.writer = SummaryWriter(log_dir=str(Path(checkpoint_dir) / "logs"))  # –Ø–≤–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            print(f"üìù –õ–æ–≥–≥–µ—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤: {checkpoint_dir}/logs")

        def init_writer(self, logs):
            pass

        def on_evaluate(self, args, state, control, **kwargs):
            """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
            metrics = kwargs.get('metrics', {})
            global_step = state.global_step

                        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            cer = metrics.get('eval_cer', float('inf'))
            predictions = metrics.get('eval_predictions', [])
            labels = metrics.get('eval_labels', [])

            # # --- –ë–õ–û–ö –ü–ï–ß–ê–¢–ò –í –ö–û–ù–°–û–õ–¨ ---
            # if len(predictions) > 0:
            #     print(f"\n--- –ü–†–ò–ú–ï–†–´ –ù–ê –®–ê–ì–ï {global_step} (CER: {cer:.4f}) ---")
            #     for i in range(min(3, len(predictions))):
            #         p_ids = predictions[i]
            #         l_ids = labels[i]
            #
            #         # –ó–∞–º–µ–Ω—è–µ–º -100 –Ω–∞ pad_token_id, —á—Ç–æ–±—ã –¥–µ–∫–æ–¥–µ—Ä –Ω–µ –≤—ã–¥–∞–ª –æ—à–∏–±–∫—É
            #         p_ids[p_ids == -100] = self.processor.tokenizer.pad_token_id
            #         l_ids[l_ids == -100] = self.processor.tokenizer.pad_token_id
            #
            #         p_text = self.processor.decode(p_ids, skip_special_tokens=True)
            #         l_text = self.processor.decode(l_ids, skip_special_tokens=True)
            #
            #         print(f"–û–ñ–ò–î–ê–ù–ò–ï: '{l_text}'")
            #         print(f"–†–ï–ê–õ–¨–ù–û–°–¢–¨: '{p_text}'")
            #         print("-" * 20)

            # –õ–æ–≥–∏—Ä—É–µ–º CER –≤ TensorBoard
            if self.writer:
                self.writer.add_scalar("Val/cer", cer, global_step)

            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –≤ TensorBoard (–≤–∫–ª–∞–¥–∫–∞ TEXT)
            if len(predictions) > 0 and self.writer:
                n_samples = min(self.num_samples, len(predictions))
                # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                indices = random.sample(range(len(predictions)), n_samples)

                for i, idx in enumerate(indices):
                    curr_p_ids = predictions[idx]
                    curr_l_ids = labels[idx]

                    curr_p_ids[curr_p_ids == -100] = self.processor.tokenizer.pad_token_id
                    curr_l_ids[curr_l_ids == -100] = self.processor.tokenizer.pad_token_id

                    pred_text = self.processor.decode(curr_p_ids, skip_special_tokens=True)
                    true_text = self.processor.decode(curr_l_ids, skip_special_tokens=True)

                    self.writer.add_text(
                        f"Val/sample_{i}",
                        f"**True:** `{true_text}`  \n**Pred:** `{pred_text}`",
                        global_step
                    )
                self.writer.flush()

            # --- –õ–û–ì–ò–ö–ê –°–û–•–†–ê–ù–ï–ù–ò–Ø –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò ---
            if cer < self.best_cer:
                self.best_cer = cer
                self.epochs_no_improve = 0
                best_model_dir = self.checkpoint_dir / f"best_cer_model_cer-{cer}"
                best_model_dir.mkdir(parents=True, exist_ok=True)

                kwargs['model'].save_pretrained(best_model_dir)
                self.processor.save_pretrained(best_model_dir)

                if args.local_rank in [-1, 0]:
                    print(f"üéØ –ù–æ–≤—ã–π –ª—É—á—à–∏–π CER: {cer:.4f}. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
            else:
                self.epochs_no_improve += 1
                if self.epochs_no_improve >= self.early_stopping_patience:
                    print(f"‚ö†Ô∏è CER –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è {self.epochs_no_improve} –ø—Ä–æ–≤–µ—Ä–æ–∫ –ø–æ–¥—Ä—è–¥")

            if args.local_rank in [-1, 0]:
                print(f"Validation @ step {global_step} - CER: {cer:.4f} | Best: {self.best_cer:.4f}")

        def on_train_end(self, args, state, control, **kwargs):
            """–ó–∞–∫—Ä—ã–≤–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
            if self.writer:
                self.writer.close()
                print("‚úÖ –õ–æ–≥–≥–µ—Ä TensorBoard –∑–∞–∫—Ä—ã—Ç")


    # –°–æ–∑–¥–∞–µ–º –Ω–∞—à –Ω–æ–≤—ã–π –∫–æ–ª–±—ç–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∞ –Ω–∞ 3000 —à–∞–≥–µ)
    freeze_callback = FreezeEncoderCallback(unfreeze_step=180)

    memory_callback = MemoryOptimizationCallback()

    callback = EnhancedValidationCallback(
        checkpoint_dir=OUTPUT_DIR,
        processor=processor,
        log_every=500,  # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–µ 200 —à–∞–≥–æ–≤
        num_samples=5,  # –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å 5 –ø—Ä–∏–º–µ—Ä–æ–≤
        early_stopping_patience=5  # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ—Å–ª–µ 5 —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π
    )

    # --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Trainer ---
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        processing_class=processor,
        callbacks=[callback, freeze_callback, memory_callback],
        data_collator=default_data_collator,
    )

    resume_training = bool(get_last_checkpoint(OUTPUT_DIR))
    trainer.train(resume_from_checkpoint=resume_training)

    # --- –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---
    print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! {datetime.now().strftime('%Y-%m-%d_%H-%M')} –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å...")
    trainer.save_model(str(OUTPUT_DIR / "best_model"))
    processor.save_pretrained(str(OUTPUT_DIR / "best_model"))
    print(f"üéâ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {OUTPUT_DIR / 'best_model'}")

if __name__ == "__main__":
    main()
